### Adam 优化算法
Adam（Adaptive Moment Estimation）是一种结合了动量法和RMSProp的优化算法，广泛应用于深度学习模型的训练中。它通过计算梯度的一阶矩（均值）和二阶矩（未中心化的方差）来动态调整每个参数的学习率，从而提高训练的效率和稳定性。
Adam优化算法的核心思想是利用梯度的均值和方差来调整每个参数的学习率，使得参数更新更加平滑和有效。具体来说，Adam算法包括以下几个步骤：
1. 初始化一阶矩估计（均值）和二阶矩估计（未中心化的方差）为零向量。
2. 对于每个时间步$t$，计算当前梯度$g_t$。
3. 更新一阶矩估计$m_t$和二阶矩估计$v_t$：
   - $m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$
   - $v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$
   其中，$\beta_1$和$\beta_2$是控制一阶矩和二阶矩衰减率的超参数，通常取值为0.9和0.999。
4. 计算偏差修正后的矩估计：
   - $\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$
   - $\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$
5. 更新参数$\theta$：
   - $\theta_t = \theta_{t-1} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \varepsilon}$
   其中，$\alpha$是学习率，$\varepsilon$是一个小常数，用于防止除零错误。

Adam优化算法的优点包括：
- 自适应学习率：每个参数都有自己的学习率，能够更好地适应不同参数的更新需求。
- 结合动量和RMSProp的优点：既考虑了梯度的方向，又考虑了梯度的幅度，提升了收敛速度和稳定性。 
- 适用于大规模数据和高维参数空间：在处理大规模数据集和复杂模型时表现出色。


