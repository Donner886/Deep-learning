{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d861578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 18:09:43.442106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12eb78a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "(2, 2)\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[1.0, 2.0], \n",
    "                 [3.0, 4.0]])\n",
    "\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408eb5cd",
   "metadata": {},
   "source": [
    "The most important attributes of a tf.Tensor are its shape, and dtype.\n",
    "- Tensor.shape\n",
    "- Tensor.dtype\n",
    "\n",
    "Tensorflow implements standard mathematical operations on tensors, as well as operations specified for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191cd4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 4.],\n",
       "       [6., 8.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X + X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce4901",
   "metadata": {},
   "source": [
    "##   TensorFlow 中 tensor 的转置（tf.transpose）逻辑说明：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1134ef",
   "metadata": {},
   "source": [
    "### TensorFlow 中 tensor 的转置（tf.transpose）逻辑说明：\n",
    "\n",
    "1. perm 的含义：perm 是一个长度为 n 的整数列表，表示输出的第 k 个维度来自输入的第 perm[k] 个维度。\n",
    "   - 因此输出张量 y 的形状满足：y.shape[k] = x.shape[perm[k]]。\n",
    "   - 默认情况下，perm 会被设置为 [n-1, n-2, ..., 0]，也就是将维度反转（对 2D 就是常规矩阵转置）。\n",
    "\n",
    "2. 索引映射（更直观）：\n",
    "   - 如果 y = tf.transpose(x, perm)，则\n",
    "     y[i0, i1, ..., i{n-1}] = x[i_{perm[0]}, i_{perm[1]}, ..., i_{perm[n-1]}]。\n",
    "   - 例如：对于 2D（矩阵），perm=[1,0]，有 y[i,j] = x[j,i]；\n",
    "     对于 3D，perm=[1,0,2]，有 y[a,b,c] = x[b,a,c]（即交换第 0、1 两个维度）。\n",
    "\n",
    "3. conjugate 参数（仅对复数类型有效）：\n",
    "   - 如果 x.dtype 是 complex64 或 complex128，且调用 tf.transpose(x, conjugate=True)，\n",
    "     那么在转置的同时还会对数值做共轭（相当于复共轭转置）。\n",
    "\n",
    "4. 使用建议与注意事项：\n",
    "   - 对于高维张量，明确写出 perm 更易读也更可控；\n",
    "   - 如果只想交换两个维度，可用 perm 指定对应位置交换；\n",
    "   - 转置通常不改变内存占用，但可能改变内存布局，影响后续操作性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be2e0a",
   "metadata": {},
   "source": [
    "## 详细演示 tf.transpose 元素转换过程\n",
    "\n",
    "### 核心原理：索引映射规则\n",
    "如果 `y = tf.transpose(x, perm)`，那么：\n",
    "**`y[i0, i1, ..., i_{n-1}] = x[i_{perm[0]}, i_{perm[1]}, ..., i_{perm[n-1]}]`**\n",
    "\n",
    "这意味着：\n",
    "- 输出张量 y 在位置 `[i0, i1, ..., i_{n-1}]` 的元素\n",
    "- 来自输入张量 x 在位置 `[i_{perm[0]}, i_{perm[1]}, ..., i_{perm[n-1]}]` 的元素\n",
    "\n",
    "下面我们用具体例子来演示这个转换过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b799f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "示例1：2D矩阵转置 perm=[1,0]\n",
      "============================================================\n",
      "原始矩阵 A (shape: 2x3):\n",
      "[[10 20 30]\n",
      " [40 50 60]]\n",
      "\n",
      "转置后 A_t (shape: 3x2):\n",
      "[[10 40]\n",
      " [20 50]\n",
      " [30 60]]\n",
      "\n",
      "元素转换过程详解:\n",
      "根据规则：A_t[i,j] = A[j,i] (因为perm=[1,0])\n",
      "\n",
      "A_t[0,0] = A[0,0] = 10 (来自 A[0,0] = 10)\n",
      "A_t[0,1] = A[1,0] = 40 (来自 A[1,0] = 40)\n",
      "A_t[1,0] = A[0,1] = 20 (来自 A[0,1] = 20)\n",
      "A_t[1,1] = A[1,1] = 50 (来自 A[1,1] = 50)\n",
      "A_t[2,0] = A[0,2] = 30 (来自 A[0,2] = 30)\n",
      "A_t[2,1] = A[1,2] = 60 (来自 A[1,2] = 60)\n",
      "\n",
      "可视化转换过程:\n",
      "A[0,0]=10 → A_t[0,0]=10\n",
      "A[0,1]=20 → A_t[1,0]=20\n",
      "A[0,2]=30 → A_t[2,0]=30\n",
      "A[1,0]=40 → A_t[0,1]=40\n",
      "A[1,1]=50 → A_t[1,1]=50\n",
      "A[1,2]=60 → A_t[2,1]=60\n"
     ]
    }
   ],
   "source": [
    "# 示例1：2D矩阵转置 - 逐步演示元素转换\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"示例1：2D矩阵转置 perm=[1,0]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 创建一个2x3的矩阵\n",
    "A = tf.constant([[10, 20, 30], \n",
    "                 [40, 50, 60]])\n",
    "print(\"原始矩阵 A (shape: 2x3):\")\n",
    "print(A.numpy())\n",
    "\n",
    "# 转置：perm=[1,0] 意味着新矩阵的维度0来自原矩阵的维度1，维度1来自原矩阵的维度0\n",
    "A_t = tf.transpose(A, perm=[1, 0])\n",
    "print(f\"\\n转置后 A_t (shape: 3x2):\")\n",
    "print(A_t.numpy())\n",
    "\n",
    "print(\"\\n元素转换过程详解:\")\n",
    "print(\"根据规则：A_t[i,j] = A[j,i] (因为perm=[1,0])\")\n",
    "print()\n",
    "\n",
    "# 逐个验证每个元素的转换\n",
    "for i in range(3):  # A_t的行\n",
    "    for j in range(2):  # A_t的列\n",
    "        print(f\"A_t[{i},{j}] = A[{j},{i}] = {A_t[i,j].numpy()} (来自 A[{j},{i}] = {A[j,i].numpy()})\")\n",
    "\n",
    "print(\"\\n可视化转换过程:\")\n",
    "print(\"A[0,0]=10 → A_t[0,0]=10\")\n",
    "print(\"A[0,1]=20 → A_t[1,0]=20\") \n",
    "print(\"A[0,2]=30 → A_t[2,0]=30\")\n",
    "print(\"A[1,0]=40 → A_t[0,1]=40\")\n",
    "print(\"A[1,1]=50 → A_t[1,1]=50\")\n",
    "print(\"A[1,2]=60 → A_t[2,1]=60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34f30c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "示例2：3D张量转置 perm=[2,1,0] - 完全反转维度\n",
      "============================================================\n",
      "原始3D张量 B (shape: 2x2x3):\n",
      "B[0,:,:] =\n",
      "[[100 101 102]\n",
      " [110 111 112]]\n",
      "B[1,:,:] =\n",
      "[[200 201 202]\n",
      " [210 211 212]]\n",
      "\n",
      "转置后 B_t (shape: 3x2x2):\n",
      "B_t[0,:,:] =\n",
      "[[100 200]\n",
      " [110 210]]\n",
      "B_t[1,:,:] =\n",
      "[[101 201]\n",
      " [111 211]]\n",
      "B_t[2,:,:] =\n",
      "[[102 202]\n",
      " [112 212]]\n",
      "\n",
      "元素转换规则：B_t[i,j,k] = B[k,j,i] (因为perm=[2,1,0])\n",
      "\n",
      "验证几个关键元素的转换:\n",
      "B_t[0,0,0] = 100 ← B[0,0,0] = 100\n",
      "B_t[0,1,1] = 210 ← B[1,1,0] = 210\n",
      "B_t[1,0,1] = 201 ← B[1,0,1] = 201\n",
      "B_t[2,1,0] = 112 ← B[0,1,2] = 112\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"示例2：3D张量转置 perm=[2,1,0] - 完全反转维度\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 创建一个2x2x3的3D张量\n",
    "B = tf.constant([[[100, 101, 102], \n",
    "                  [110, 111, 112]], \n",
    "                 [[200, 201, 202], \n",
    "                  [210, 211, 212]]])\n",
    "\n",
    "print(\"原始3D张量 B (shape: 2x2x3):\")\n",
    "print(\"B[0,:,:] =\")\n",
    "print(B[0,:,:].numpy())\n",
    "print(\"B[1,:,:] =\") \n",
    "print(B[1,:,:].numpy())\n",
    "\n",
    "# 转置：perm=[2,1,0] 意味着：\n",
    "# - 新张量的维度0来自原张量的维度2\n",
    "# - 新张量的维度1来自原张量的维度1  \n",
    "# - 新张量的维度2来自原张量的维度0\n",
    "B_t = tf.transpose(B, perm=[2, 1, 0])\n",
    "print(f\"\\n转置后 B_t (shape: 3x2x2):\")\n",
    "print(\"B_t[0,:,:] =\")\n",
    "print(B_t[0,:,:].numpy())\n",
    "print(\"B_t[1,:,:] =\")\n",
    "print(B_t[1,:,:].numpy()) \n",
    "print(\"B_t[2,:,:] =\")\n",
    "print(B_t[2,:,:].numpy())\n",
    "\n",
    "print(\"\\n元素转换规则：B_t[i,j,k] = B[k,j,i] (因为perm=[2,1,0])\")\n",
    "print(\"\\n验证几个关键元素的转换:\")\n",
    "\n",
    "# 选择几个具体位置验证\n",
    "test_positions = [(0,0,0), (0,1,1), (1,0,1), (2,1,0)]\n",
    "\n",
    "for i, j, k in test_positions:\n",
    "    original_val = B[k, j, i].numpy()  # 根据perm=[2,1,0]，B_t[i,j,k] = B[k,j,i]\n",
    "    transposed_val = B_t[i, j, k].numpy()\n",
    "    print(f\"B_t[{i},{j},{k}] = {transposed_val} ← B[{k},{j},{i}] = {original_val}\")\n",
    "    assert transposed_val == original_val, \"转换验证失败！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b18187d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "示例3：3D张量部分维度交换 perm=[1,0,2]\n",
      "============================================================\n",
      "原始3D张量 C (shape: 2x3x2):\n",
      "可以理解为2个3x2的矩阵:\n",
      "C[0,:,:] =\n",
      "[[100 101]\n",
      " [110 111]\n",
      " [120 121]]\n",
      "C[1,:,:] =\n",
      "[[200 201]\n",
      " [210 211]\n",
      " [220 221]]\n",
      "\n",
      "转置后 C_t (shape: 3x2x2):\n",
      "现在变成3个2x2的矩阵:\n",
      "C_t[0,:,:] =\n",
      "[[100 101]\n",
      " [200 201]]\n",
      "C_t[1,:,:] =\n",
      "[[110 111]\n",
      " [210 211]]\n",
      "C_t[2,:,:] =\n",
      "[[120 121]\n",
      " [220 221]]\n",
      "\n",
      "元素转换规则：C_t[i,j,k] = C[j,i,k] (因为perm=[1,0,2])\n",
      "详细转换过程:\n",
      "C_t[0,0,0] = 100 ← C[0,0,0] = 100\n",
      "C_t[0,0,1] = 101 ← C[0,0,1] = 101\n",
      "C_t[0,1,0] = 200 ← C[1,0,0] = 200\n",
      "C_t[0,1,1] = 201 ← C[1,0,1] = 201\n",
      "C_t[1,0,0] = 110 ← C[0,1,0] = 110\n",
      "C_t[1,0,1] = 111 ← C[0,1,1] = 111\n",
      "C_t[1,1,0] = 210 ← C[1,1,0] = 210\n",
      "C_t[1,1,1] = 211 ← C[1,1,1] = 211\n",
      "C_t[2,0,0] = 120 ← C[0,2,0] = 120\n",
      "C_t[2,0,1] = 121 ← C[0,2,1] = 121\n",
      "C_t[2,1,0] = 220 ← C[1,2,0] = 220\n",
      "C_t[2,1,1] = 221 ← C[1,2,1] = 221\n",
      "\n",
      "观察：第2维（最后一维）的元素保持在相同位置，只有前两维进行了交换\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"示例3：3D张量部分维度交换 perm=[1,0,2]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 创建一个更直观的3D张量，每个元素包含其坐标信息\n",
    "C = tf.constant([[[100, 101], [110, 111], [120, 121]], \n",
    "                 [[200, 201], [210, 211], [220, 221]]])\n",
    "\n",
    "print(\"原始3D张量 C (shape: 2x3x2):\")\n",
    "print(\"可以理解为2个3x2的矩阵:\")\n",
    "for i in range(2):\n",
    "    print(f\"C[{i},:,:] =\")\n",
    "    print(C[i,:,:].numpy())\n",
    "\n",
    "# perm=[1,0,2] 意味着只交换第0和第1维，第2维保持不变\n",
    "C_t = tf.transpose(C, perm=[1, 0, 2])\n",
    "print(f\"\\n转置后 C_t (shape: 3x2x2):\")\n",
    "print(\"现在变成3个2x2的矩阵:\")\n",
    "for i in range(3):\n",
    "    print(f\"C_t[{i},:,:] =\")\n",
    "    print(C_t[i,:,:].numpy())\n",
    "\n",
    "print(\"\\n元素转换规则：C_t[i,j,k] = C[j,i,k] (因为perm=[1,0,2])\")\n",
    "print(\"详细转换过程:\")\n",
    "\n",
    "# 展示所有元素的转换\n",
    "for i in range(3):  # 新张量的第0维\n",
    "    for j in range(2):  # 新张量的第1维  \n",
    "        for k in range(2):  # 新张量的第2维\n",
    "            original_val = C[j, i, k].numpy()  # C_t[i,j,k] = C[j,i,k]\n",
    "            transposed_val = C_t[i, j, k].numpy()\n",
    "            print(f\"C_t[{i},{j},{k}] = {transposed_val:3d} ← C[{j},{i},{k}] = {original_val:3d}\")\n",
    "\n",
    "print(\"\\n观察：第2维（最后一维）的元素保持在相同位置，只有前两维进行了交换\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4702ddc",
   "metadata": {},
   "source": [
    "### 总结：tf.transpose 元素转换的核心规律\n",
    "\n",
    "1. **索引映射公式**：`y[i0, i1, ..., i_{n-1}] = x[i_{perm[0]}, i_{perm[1]}, ..., i_{perm[n-1]}]`\n",
    "\n",
    "2. **维度变换**：`y.shape[k] = x.shape[perm[k]]`\n",
    "\n",
    "3. **直观理解**：\n",
    "   - `perm[k]` 告诉我们新张量的第k维来自原张量的第`perm[k]`维\n",
    "   - 每个元素都会根据这个映射规则\"搬家\"到新位置\n",
    "\n",
    "4. **常见用法**：\n",
    "   - `perm=[1,0]`：2D矩阵的标准转置\n",
    "   - `perm=[2,1,0]`：完全反转所有维度\n",
    "   - `perm=[1,0,2]`：只交换前两维，保持其他维度不变\n",
    "\n",
    "下面运行一个交互式的验证工具："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd55344",
   "metadata": {},
   "source": [
    "##  Tenforflow concat \n",
    "\n",
    "Concatenates tensors along one dimension.  Concatenates the list of tensors values along dimension axis. if value[i].shape = [D0,D1,...,Daxis,...,Dn], the concatenated result has shape [D0,D1,...,Raxis,...,Dn], where Raxis = sum of Daxis over all tensors.\n",
    "\n",
    "This is, the data from the input tensors is joined along the axis dimension.\n",
    "the number of dimensions of the input tensors must match, and all dimensions except axis must be the same size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 2)\n",
      "tf.Tensor(\n",
      "[[ 1  2  3  7  8]\n",
      " [ 4  5  6 10 11]], shape=(2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t1 = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "print(t1.shape)\n",
    "\n",
    "t2 = tf.constant([[7, 8], [10, 11]])\n",
    "print(t2.shape)\n",
    "\n",
    "result = tf.concat([t1, t2], axis=1)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c0f25",
   "metadata": {},
   "source": [
    "## TensorFlow 中 tensor softmax(x, axis=-1)\n",
    "\n",
    "Used for multi-class prediction. The sum of all outputs generated by softmax is 1.\n",
    "\n",
    "This function performs the equivalent of the following \n",
    "\n",
    "$$softmax = tf.exp(x) / tf.reduce\\_sum(tf.exp(x), axis=axis, keepdims=True)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03e10841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.09003057 0.24472848 0.66524094], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = tf.nn.softmax([-1, 0., 1.])\n",
    "print(softmax)\n",
    "\n",
    "sum(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057b192",
   "metadata": {},
   "source": [
    "####  TensorFlow 中 exponentation 函数 tf.exp(x)\n",
    "\n",
    "Computes the exponential of x element-wise. y = e^x\n",
    "\n",
    "This function computes the exponential of each element in the input tensor x. i.e. math.exp(x) or e^x. where x is the input tensor. \n",
    "e denotes Euler's number and is approximately equal to 2.718281828459045. Output is positive for any real input.  \n",
    "\n",
    "\n",
    "For complex(复数) numbers, the exponential function value is calculated as follows:\n",
    "$$ e^(a+bi) = e^a * (cos(b) + i*sin(b)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b366f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7.389056, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[  2.7182817   7.389056   20.085537 ]\n",
      " [ 54.59815   148.41316   403.4288   ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(2.0)\n",
    "print(tf.exp(x))\n",
    "\n",
    "x = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "print(tf.exp(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea71eb",
   "metadata": {},
   "source": [
    "####   TensorFlow math reduce_sum(input_tensor, axis=None, keepdims=False)\n",
    "\n",
    "Computes the sum of elements across dimensions of a tensor.\n",
    "\n",
    "This is the reduction operation for the elementwise tf.math.add op.\n",
    "\n",
    "reduces input_tensor along the dimensions given in axis. unless keepdims is true, the rank of the tensor is reduced by 1 for each entry in axis, which must be unique. if keepdims is true, the reduced dimensions are retained with length 1. For purpose of consistency of output shape, prefer to keepdims=True.\n",
    "\n",
    "if axis is None, all dimensions are reduced, and a tensor with a single element is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f7e2030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 1 1]\n",
      " [1 1 1]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor([2 2 2], shape=(3,), dtype=int32)\n",
      "tf.Tensor([3 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[3]\n",
      " [3]], shape=(2, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# x has a shape of (2, 3) (two rows and three columns):\n",
    "x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "print(x)\n",
    "## sum all the elements \n",
    "print(tf.reduce_sum(x))\n",
    "\n",
    "## Reduce along the first dimension\n",
    "print(tf.reduce_sum(x, axis=0))  # shape (3,)\n",
    "\n",
    "## Reduce along the second dimension\n",
    "## keepdims = True 保持降维后的维度信息\n",
    "print(tf.reduce_sum(x, axis=1))  # shape (2,)\n",
    "\n",
    "## Reduce along the second dimension and keepdims=True\n",
    "print(tf.reduce_sum(x, axis=1, keepdims=True))  # shape (2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb498a",
   "metadata": {},
   "source": [
    "## TensorFlow中convert_to_tensor\n",
    "\n",
    "Converts the given value to a Tensor.\n",
    "\n",
    "This function converts python objects of various types to tensor objects. It accepts Tensor objects, numpy arrarys, python lists, python scalars and more.\n",
    "\n",
    "this function can be useful to when composing a new operation in python. All standard Python op constructors apply this function to each of their Tensor-valued inputs, which allows those ops to accept numpy arrarys, python lists, and scalars as inputs in addition to Tensor objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def my_func(arg):\n",
    "    arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b4bb7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3],[4, 5, 6]], dtype=np.float32)\n",
    "print(x)\n",
    "\n",
    "value_1 = my_func(x)\n",
    "print(value_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5eb8cb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow **IS NOT** using the GPU\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "  print(\"TensorFlow **IS** using the GPU\")\n",
    "else:\n",
    "  print(\"TensorFlow **IS NOT** using the GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31c7437c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade0283b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0992edb9",
   "metadata": {},
   "source": [
    "## TensorFlow Variables\n",
    "\n",
    "Normal tf.Tensor objects are immutable. To store model weights or other mutable state in TensorFlow use a  tf.Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7ef2d910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[1., 2.],\n",
      "       [3., 4.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[5., 6.],\n",
      "       [7., 8.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[6., 7.],\n",
      "       [8., 9.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "var = tf.Variable([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(var)\n",
    "\n",
    "var.assign([[5.0, 6.0], [7.0, 8.0]])\n",
    "print(var)\n",
    "\n",
    "var.assign_add([[1.0, 1.0], [1.0, 1.0]])\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045de3d",
   "metadata": {},
   "source": [
    "## Automatic differentiation（微分， 求导的过程，这个过程用于确定函数在某个点上瞬时的变换率或斜率）\n",
    "Gradient descent and related algorithms are a cornerstore (指的是建筑物底部或地基中放置的第一块石头， 通常有象征意义用来说明事物基础或者核心的部分) of modern machine learning . \n",
    "\n",
    "To enable this, Tenforflow implements automatic differentiation(autodiff), which uses calculus to compute gradients.  Typically, you'll use this to caluculate the gradient of a model's error or loss with respect to its weights. \n",
    "\n",
    "The simplified example only takes the derivative with respect to a single scalar variable x, but Tensorflow can compute the gradient with respect to any number of non-scalar tensors simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "70d00be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(1.0)\n",
    "def f(x):\n",
    "    y = x**2 + 2*x - 5\n",
    "    return y\n",
    "\n",
    "f(x)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)\n",
    "\n",
    "g_x = tape.gradient(y, x) ## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81881552",
   "metadata": {},
   "source": [
    "### Introduction to gradients and automatic differentiation\n",
    "\n",
    "Automatic Differentiation is useful for implementing machine learning algorithms such as bacpropagation for training neural networks. \n",
    "\n",
    "In this guide, you will explore ways to compute gradients with Tensorflow, expecially in eager execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2b4b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e6bb7",
   "metadata": {},
   "source": [
    "#### Computing gradients\n",
    "\n",
    "To differentiate automatically, Tensorflow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, Tensorflow traverses this list of operations in reverse order to compute the gradients.  \n",
    "\n",
    "### Gradient Tapes\n",
    "Tensorflow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, ususally tf.Variables. Tensorflow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". Tensorflow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation.\n",
    "\n",
    "**翻译为中文：**\n",
    "TensorFlow 提供了 tf.GradientTape API 用于自动微分；也就是计算某个计算相对于一些输入（通常是 tf.Variables）的梯度。TensorFlow 会在 tf.GradientTape 的上下文中“记录”执行的相关操作到一个“磁带”上。然后，TensorFlow 使用该磁带通过反向模式微分来计算“记录”的计算的梯度。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8896ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2 + 2*x - 5\n",
    "\n",
    "### Once you've recorded some operations, use GrandientTape.gradient() to calculate the gradient of some target (often a loss) relative to some source (often model's variables)\n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()\n",
    "print(dy_dx.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "290d2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### the above example uses scalars, but tf.GradientTape works as easily on any tensors. \n",
    "# 标准神经网络表示法：w(n_l, n_l-1), b(n_l)\n",
    "# n_l-1 = 3 (前一层/输入层神经元数)\n",
    "# n_l = 2 (当前层神经元数)\n",
    "w = tf.Variable(tf.random.normal((2, 3)), name='w')  # (n_l, n_l-1) = (2, 3)\n",
    "b = tf.Variable(tf.zeros((2,)), name='b')           # (n_l,) = (2,)\n",
    "x = [[1.0, 2.0, 3.0],\n",
    "     [4.0, 5.0, 6.0]]\n",
    "with tf.GradientTape() as tape:\n",
    "    y = w @ tf.transpose(x) + tf.expand_dims(b, axis=1)  # 标准神经网络前向传播\n",
    "    loss = tf.reduce_mean(y**2)  # Mean squared error loss\n",
    "    \n",
    "    \n",
    "## to get the gradients of loss with respect to both variables w and b, you can pass both as sources to the gradient method. the tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradients structured the same way.  \n",
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
    "print(\"Gradient wrt w:\")\n",
    "print(dl_dw.numpy())\n",
    "print(\"Gradient wrt b:\")\n",
    "print(dl_db.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is the gradient calculation again, this time passing a dictionary of variables as sources.\n",
    "with tf.GradientTape() as tape:\n",
    "    y = w @ tf.transpose(x) + tf.expand_dims(b, axis=1)  # 标准神经网络前向传播\n",
    "    loss = tf.reduce_mean(y**2)  # Mean squared error loss\n",
    "    \n",
    "my_vars = {'weight': w, 'bias': b}\n",
    "grads = tape.gradient(loss, my_vars)\n",
    "print(\"Gradient wrt weight:\")\n",
    "print(grads['weight'].numpy())\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(grads['bias'].numpy())    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b848ce8",
   "metadata": {},
   "source": [
    "#### Gradients with respect to a model\n",
    "It's common to collect tf.Variables into a tf.Module or one of its subclasses (layers.Layer, keras.Model) for checkpointing and exporting. \n",
    "\n",
    "In most cases, you will want to calculate gradients with respect to a model's trainable variables. Since all subclasses of tf.Module aggregate their variables in the Module.trainable_variables property, you can calculate these gradients in a few lines of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64df5ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Variable: kernel, Gradient: [[3.1209607 0.5997902]\n",
      " [6.2419214 1.1995804]\n",
      " [9.362883  1.7993706]]\n",
      "Variable: bias, Gradient: [3.1209607 0.5997902]\n"
     ]
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(units=2, activation='relu')  # 输入大小3，输出大小2\n",
    "x = tf.constant([[1.0, 2.0, 3.0]])\n",
    "\n",
    "print(layer.trainable_variables)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = layer(x)\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "    \n",
    "## calculate gradients with respect to the layer's trainable variables\n",
    "grads = tape.gradient(loss, layer.trainable_variables)\n",
    "for var, grad in zip(layer.trainable_variables, grads):\n",
    "    print(f\"Variable: {var.name}, Gradient: {grad.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3e764",
   "metadata": {},
   "source": [
    "### Controlling what the tape watches\n",
    "The default behavior is to record all operations after accessing a trainable tf.Variable. The reasons for this are:\n",
    "- The tape needs to know which operations to record in the forward pass to calculate the gradients in the backward pass.\n",
    "- The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
    "- The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
    "\n",
    "For example, the following fails to calculate a gradient because the tf.Tensor is not watches by default, and the tf.Variable is not trainable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c48a146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient wrt x0: 6.0\n",
      "Gradient wrt x1: None\n",
      "Gradient wrt x2: None\n",
      "Gradient wrt x3: None\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# A non-trainable variable\n",
    "x1 = tf.Variable(3.0, trainable=False, name='x1')\n",
    "# Not a variable: A variable + tensor returns a tensor\n",
    "x2 = tf.Variable(3.0) + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x0**2 + x1**2 + x2**2 + x3**2\n",
    "\n",
    "grads = tape.gradient(y, [x0, x1, x2, x3])\n",
    "for i, grad in enumerate(grads):\n",
    "    print(f\"Gradient wrt x{i}: {grad}\")\n",
    "    \n",
    "## you can list the variables being watched using the GradientTape.watched_variables() method.\n",
    "[var.name for var in tape.watched_variables()]\n",
    "\n",
    "## tf.GradientTape provides hooks that give the user control over what is or is not watched.\n",
    "## To record gradients with respect to a tf.Tensor, use the watch() method:\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)  # Explicitly watch the tensor x\n",
    "    y = x**2\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06af830e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient wrt x0: -0.9899924993515015\n",
      "Gradient wrt x1: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Conversely, to disable the default behavior of watching all tf.Variables, set watch_accessed_variables=False when creating the tape. This calcualtion uses two variables,  but only connects the gradient for one of the variables:\n",
    "\n",
    "'''\n",
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(4.0)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(x0)  # Only watch x0\n",
    "    y0 = tf.math.sin(x0)\n",
    "    y1 = tf.nn.softplus(x1)\n",
    "    y = y0 + y1\n",
    "    ys = tf.reduce_sum(y)\n",
    "\n",
    "## since GradientTape.watch was ont called on x1, no gradient is computed with respect to it.\n",
    "\n",
    "grads = tape.gradient(ys, {'x0': x0, 'x1': x1 })\n",
    "print(f\"Gradient wrt x0: {grads['x0'].numpy()}\")\n",
    "print(f\"Gradient wrt x1: {grads['x1']}\")  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f9de695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n",
      "[  4. 108.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "### Intermediate results\n",
    "\n",
    "\"\"\"\n",
    "You can also request gradients of the output with respect to intermediate values computed inside the tf.GradientTape context by passing in those intermediate values as sources to the gradient() method.\n",
    "\n",
    "By default, the resources held by a GradientTape are released as soon as the GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent=True. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. \n",
    "\"\"\"\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x \n",
    "    z = y * y\n",
    "    \n",
    "# use the tape to compute the gradient of z with respect to intermediate value y\n",
    "dz_dy = tape.gradient(z, y)\n",
    "print(dz_dy.numpy())  # dz/dy = 2*y = 2\n",
    "\n",
    "\n",
    "x = tf.constant([1,3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x \n",
    "    z = y * y\n",
    "print(tape.gradient(z, x).numpy())  # dz/dx = 4*x^3 = 4*3^3 = 108\n",
    "print(tape.gradient(y, x).numpy())  # dy/dx = 2*x = 2*3 = 6\n",
    "del tape  # 删除tape以释放资源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f8af8",
   "metadata": {},
   "source": [
    "### Notes on performance\n",
    "- There is a tiny overhead associated with doing operations inside a gradient tape context. for most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required. \n",
    "- Gradient tapes use memory to store intermediate result, including inputs and outputs, for use during the backward pass.\n",
    "\n",
    "For efficiency, some ops (liek RelU) don't need to keep their intermediate reuslts and they are pruned during the forward pass. However, if you use persistent=True on your tape, nothing is discarded and your peak memory usage will be higher.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52305e9",
   "metadata": {},
   "source": [
    "## Gradients on non-scalar targets\n",
    "\n",
    "A gradient is fundamentally an operation on a scalar.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de2d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-0.25\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y0 = x**2\n",
    "    y1 = 1 / x\n",
    "    \n",
    "print(tape.gradient(y0, x).numpy())  # dy0/dx = 2*x = 4\n",
    "print(tape.gradient(y1, x).numpy())  # dy1/dx = -1/x^2 = -1/4\n",
    "\n",
    "del tape  # 删除tape以释放资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e54b74b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n",
      "6.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Thus, if you ask for the gradient of multiple targets, the result for each source is :\n",
    "- The gradient of the sum of the targets, or equivalently,\n",
    "- The sum of the gradients of each target. \n",
    "\"\"\"\n",
    "\n",
    "x  = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y0 = x**2\n",
    "    y1 = 1 / x\n",
    "    y = y0 + y1\n",
    "\n",
    "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())  # dy/dx = dy0/dx + dy1/dx = 2*x - 1/x^2 = 4 - 1/4 = 15/4\n",
    "# print(tape.gradient(y, x).numpy())  # dy/dx = dy0/dx + dy1/dx = 2*x - 1/x^2 = 4 - 1/4 = 15/4\n",
    "\n",
    "\n",
    "## Similarly, if the target(s) are not scalar, the gradient of the sum is calculated:\n",
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x * [1.0, 2.0, 3.0]  # y is a vector now\n",
    "\n",
    "print(tape.gradient(y, x).numpy())  # dy/dx = [1.0, 2.0, 3.0], sum = 6.0\n",
    "\n",
    "\"\"\"\n",
    "This makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of element-wise loss calculations.  \n",
    "if you need a seperate gradient for each item, refer to Jacobians. \n",
    "In some cases, you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "x  = tf.linspace(-10, 10, 200+1)  \n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.nn.sigmoid(x)\n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "plt.plot(x, y, label='y')\n",
    "plt.plot(x, dy_dx, label=\"sigmoid\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756ac82",
   "metadata": {},
   "source": [
    "### Control flow\n",
    "\n",
    "Because a gradient tape records operations as they are executed, python control flow is natually handled (for example, if and while statements).\n",
    "\n",
    "Here a different variable is used on each branch of an if. The gradient only connects to the variable that was used. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96776894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient wrt v0: 1.0\n",
      "Gradient wrt v1: None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.0)\n",
    "\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    if x > 0:\n",
    "        result =  v0\n",
    "    else:\n",
    "        result = v1**2\n",
    "        \n",
    "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
    "print(f\"Gradient wrt v0: {dv0.numpy()}\")  # Should be 1.0\n",
    "print(f\"Gradient wrt v1: {dv1}\")  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e72b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187998a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d300a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97396530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448503c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7338db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decaf0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3003da5e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4167c1c6",
   "metadata": {},
   "source": [
    "### TensorFlow Module \n",
    "\n",
    "Base neural network module class.\n",
    "\n",
    "A module is a named container for tf.Variables, other tf.Modules and functions which apply to user input. For example a dense layer in a neural network might be implemented as a tf.Module:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'w:0' shape=(2, 3) dtype=float32, numpy=\n",
       " array([[-0.3479347 , -0.63158643, -1.2771387 ],\n",
       "        [ 0.0831783 , -0.16191687, -0.04580844]], dtype=float32)>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dense(tf.Module):\n",
    "    def __init__(self, input_size, output_size, name=None):\n",
    "        super().__init__(name=name)\n",
    "        # 初始化权重和偏置\n",
    "        self.w = tf.Variable(tf.random.normal((output_size, input_size)), name='w')\n",
    "        self.b = tf.Variable(tf.zeros((output_size,)), name='b')\n",
    "    '''\n",
    "    `__call__` 是Python中的一个**特殊方法**（magic method），它使得一个对象可以像函数一样被调用。\n",
    "\n",
    "    ### 基本原理：\n",
    "    - 当你定义了 `__call__` 方法后，对象实例就变成了**可调用对象**（callable）\n",
    "    - 你可以直接使用 `对象名()` 的语法来调用对象，就像调用函数一样\n",
    "    - 实际上会自动执行对象的 `__call__` 方法 \n",
    "    '''\n",
    "    def __call__(self, x):\n",
    "        # 前向传播\n",
    "        y = tf.matmul(self.w, tf.transpose(x)) + self.b[:, tf.newaxis]\n",
    "        return tf.nn.relu(y)  # 使用ReLU激活函数\n",
    "    \n",
    "    \n",
    "## you can use the Dense layer as you would expect:\n",
    "d = Dense(input_size=3, output_size=2)  # 输入大小3，输出大小2\n",
    "d(tf.ones([1,3]))\n",
    "\n",
    "## By subclassing tf.Module instead of object any tf.Variables or tf.Module instances assigned to object properties can be collected using the variables, trainable_variables or submodules methods. This makes it easy to build complex models out of simple, reusable pieces.\n",
    "\n",
    "d.variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9cc6b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'w:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[-1.0387961e+00,  6.2589651e-01, -1.3271900e+00],\n",
       "        [ 3.8544044e-01,  3.4829724e-01,  5.6471926e-01],\n",
       "        [-5.8467859e-01,  2.9635718e-01,  5.9866445e-04],\n",
       "        [ 1.2745401e-01, -1.3853037e+00, -1.2565813e+00],\n",
       "        [-7.9661018e-01,  2.3742171e-02,  2.8781995e-01]], dtype=float32)>,\n",
       " <tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'w:0' shape=(2, 5) dtype=float32, numpy=\n",
       " array([[-0.75878435, -0.33371702,  0.7965835 ,  1.2078881 , -1.0022925 ],\n",
       "        [ 1.9517893 , -1.3006377 ,  0.49805993,  0.6341641 ,  0.9978493 ]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Subclasses of tf.Module can take advantage of the _flatten method which can be used to implement tracking of any other types. \n",
    "## All tf.Module classes have an associated tf.name_scope which can be used to group operations in TensorBoard and create hierarchical names which can help with debugging. we suggest using the name scope when creating nested submodules/parameters or for forward methods whose graph you might want to inspect in TensorBoard. you can enter the name scope explicitly using a with self.name_scope(): block.or you can annotate methods (apart from __init__) with @tf.Module.with_name_scope to have them automatically enter the name scope when called.\n",
    "\n",
    "\n",
    "class MLP(tf.Module):\n",
    "    def __init__(self, input_size, sizes, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.layers = []\n",
    "        for i, size in enumerate(sizes):\n",
    "            layer = Dense(input_size=input_size, output_size=size, name=f'dense_{i}')\n",
    "            self.layers.append(layer)\n",
    "            input_size = size\n",
    "            \n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "module = MLP(input_size=3, sizes=[5, 2])\n",
    "module.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f3be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabab12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da7322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7327d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ffd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f5ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4c638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e1107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3974f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf48f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590eee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad893ca0",
   "metadata": {},
   "source": [
    "## TensorFlow modules, layers, and models\n",
    "tf.Module is a class for managing your tf.Variables objects, and the tf.function objects that operate on them. \n",
    "The tf.Module class is necessary to support two significant features:\n",
    "\n",
    "1. You can save and restore the values of your variables using tf.train.Checkpoint. This is useful during training as it is quick to save and restore a model's state.\n",
    "2. You can import and export the tf.Variables value and the tf.function graphs using tf.saved_model. this is allows you to run your model independently of python pragram that created it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cee21",
   "metadata": {},
   "source": [
    "### TensorFlow Training loops\n",
    "\n",
    "Now put this all together to build a basic model and train it from scratch.\n",
    "\n",
    "First, create some example data. This generates a cloud of points that loosely follows a quadrtic curve. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b165c87",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
